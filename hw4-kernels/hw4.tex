%% LyX 2.1.0 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[ruled]{article}
\usepackage{courier}
\usepackage[latin9]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose}
\usepackage{url}
\usepackage{algorithm2e}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage[unicode=true,
 bookmarks=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=false]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
 \theoremstyle{definition}
 \newtheorem*{defn*}{\protect\definitionname}
  \theoremstyle{plain}
  \newtheorem*{thm*}{\protect\theoremname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\title{Machine Learning and Computational Statistics, Spring 2015\\
Homework 4: Kernels and Duals} 
\date{}
%\date{Due: February $30^{th}$, 4pm}




\usepackage{amsfonts}\usepackage{capt-of}
%\usepackage{url}
\usepackage{graphicx}
\usepackage{color}
\usepackage{bbm}
\usepackage{enumerate}
\newcommand{\carlos}[1]{\textcolor{red}{Carlos: #1}}
\newcommand{\field}[1]{\mathbb{#1}} 
\newcommand{\hide}[1]{#1}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\providecommand{\m}[1]{\mathbf{#1}}
\providecommand{\norm}[1]{\left\|#1\right\|}
\providecommand{\sign}[1]{\text{sign}\left(#1\right)}
\DeclareMathOperator*{\argmin}{arg\,min}
\providecommand{\what}{\m{\hat{w}}}
\providecommand{\dw}{\Delta w}
\providecommand{\dmw}{\Delta \m{w}}
\providecommand{\hy}{\hat{y}}

\makeatother

  \providecommand{\definitionname}{Definition}
  \providecommand{\theoremname}{Theorem}

\begin{document}
\global\long\def\reals{\mathbf{R}}
 \global\long\def\integers{\mathbf{Z}}
\global\long\def\naturals{\mathbf{N}}
 \global\long\def\rationals{\mathbf{Q}}
\global\long\def\ca{\mathcal{A}}
\global\long\def\cb{\mathcal{B}}
 \global\long\def\cc{\mathcal{C}}
 \global\long\def\cd{\mathcal{D}}
\global\long\def\ce{\mathcal{E}}
\global\long\def\cf{\mathcal{F}}
\global\long\def\cg{\mathcal{G}}
\global\long\def\ch{\mathcal{H}}
\global\long\def\ci{\mathcal{I}}
\global\long\def\cj{\mathcal{J}}
\global\long\def\ck{\mathcal{K}}
\global\long\def\cl{\mathcal{L}}
\global\long\def\cm{\mathcal{M}}
\global\long\def\cn{\mathcal{N}}
\global\long\def\co{\mathcal{O}}
\global\long\def\cp{\mathcal{P}}
\global\long\def\cq{\mathcal{Q}}
\global\long\def\calr{\mathcal{R}}
\global\long\def\cs{\mathcal{S}}
\global\long\def\ct{\mathcal{T}}
\global\long\def\cu{\mathcal{U}}
\global\long\def\cv{\mathcal{V}}
\global\long\def\cw{\mathcal{W}}
\global\long\def\cx{\mathcal{X}}
\global\long\def\cy{\mathcal{Y}}
\global\long\def\cz{\mathcal{Z}}
\global\long\def\ind#1{1(#1)}
\global\long\def\pr{\mathbb{P}}
\global\long\def\predsp{\cy}
\global\long\def\outsp{\cy}
\global\long\def\prxy{P_{\cx\times\cy}}
\global\long\def\prx{P_{\cx}}
\global\long\def\prygivenx{P_{\cy\mid\cx}}
\global\long\def\ex{\mathbb{E}}
\global\long\def\var{\textrm{Var}}
\global\long\def\cov{\textrm{Cov}}
\global\long\def\sgn{\textrm{sgn}}
\global\long\def\sign{\textrm{sign}}
\global\long\def\kl{\textrm{KL}}
\global\long\def\law{\mathcal{L}}
\global\long\def\eps{\varepsilon}
\global\long\def\as{\textrm{ a.s.}}
\global\long\def\io{\textrm{ i.o.}}
\global\long\def\ev{\textrm{ ev.}}
\global\long\def\convd{\stackrel{d}{\to}}
\global\long\def\eqd{\stackrel{d}{=}}
\global\long\def\del{\nabla}
\global\long\def\loss{\ell}
\global\long\def\risk{R}
\global\long\def\emprisk{\hat{R}_{\ell}}
\global\long\def\lossfnl{L}
\global\long\def\emplossfnl{\hat{L}}
\global\long\def\empminimizer#1{\hat{#1}_{\ell}}
\global\long\def\minimizer#1{#1_{*}}
\global\long\def\etal{\textrm{et. al.}}
\global\long\def\tr{\operatorname{tr}}
\global\long\def\trace{\operatorname{trace}}
\global\long\def\diag{\text{diag}}
\global\long\def\rank{\text{rank}}
\global\long\def\linspan{\text{span}}
\global\long\def\proj{\text{Proj}}
\global\long\def\argmax{\operatornamewithlimits{arg\, max}}
\global\long\def\argmin{\operatornamewithlimits{arg\, min}}
\global\long\def\bfx{\mathbf{x}}
\global\long\def\bfy{\mathbf{y}}
\global\long\def\bfl{\mathbf{\lambda}}
\global\long\def\bfm{\mathbf{\mu}}
\global\long\def\calL{\mathcal{L}}
\global\long\def\vw{\boldsymbol{w}}
\global\long\def\vx{\boldsymbol{x}}
\global\long\def\vxi{\boldsymbol{\xi}}
\global\long\def\valpha{\boldsymbol{\alpha}}
\global\long\def\vbeta{\boldsymbol{\beta}}
\global\long\def\vsigma{\boldsymbol{\sigma}}
\global\long\def\vmu{\boldsymbol{\mu}}
\global\long\def\vtheta{\boldsymbol{\theta}}
\global\long\def\vd{\boldsymbol{d}}
\global\long\def\vs{\boldsymbol{s}}
\global\long\def\vt{\boldsymbol{t}}
\global\long\def\vh{\boldsymbol{h}}
\global\long\def\ve{\boldsymbol{e}}
\global\long\def\vf{\boldsymbol{f}}
\global\long\def\vg{\boldsymbol{g}}
\global\long\def\vz{\boldsymbol{z}}
\global\long\def\vk{\boldsymbol{k}}
\global\long\def\va{\boldsymbol{a}}
\global\long\def\vb{\boldsymbol{b}}
\global\long\def\vv{\boldsymbol{v}}
\global\long\def\vy{\boldsymbol{y}}
\global\long\def\hil{\ch}
\global\long\def\rkhs{\hil}
\maketitle

\textbf{Due: Tuesday, March 3, 2015, at 4pm (Submit via NYU Classes)}

\textbf{Instructions}: Your answers to the questions below, including
plots and mathematical work, should be submitted as a single PDF file.
You may include your code inline or submit it as a separate file.
You may either scan hand-written work or, preferably, write your answers
using software that typesets mathematics (e.g. \LaTeX, \LyX{}, or
MathJax via iPython). 


\section{Introduction}

The problem set begins with a review of some important linear algebra
concepts that we routinely use in machine learning and statistics.
The solutions to each of these problems is at most a few lines long,
and we've tried to give helpful hints. Everything leads up to proving
a basic and important property of positive semidefinite matrices.
These aren't meant to be challenging problems -- just the opposite,
in fact -- we'd like this material to be second nature to you. We
next have a couple problems on kernel methods, both of which should
be quite easy if you understand the context. The last problem is both
an introduction to ``novelty detection'' algorithms, as well as
an exercise in the machinery of Lagrangian duality. This is the only
problem that has a lengthy solution. 


\section{Positive Semidefinite Matrices}

In statistics and machine learning, we use positive semidefinite matrices
a lot. Let's recall some definitions from linear algebra that will
be useful here:


\begin{defn*}
A set of vectors $\left\{ x_{1},\ldots,x_{n}\right\} $ is \textbf{orthonormal}
if $\left\langle x_{i},x_{i}\right\rangle =1$ for any $i\in\left\{ 1,\ldots,n\right\} $
(i.e. $x_{i}$ has unit norm), and for any $i,j\in\left\{ 1,\ldots,n\right\} $
with $i\neq j$ we have $\left\langle x_{i},x_{j}\right\rangle =0$
(i.e. $x_{i}$ and $x_{j}$ are orthogonal).

Note that if the vectors are column vectors in a Euclidean space,
we can write this as $x_{i}^{T}x_{j}=\ind{i\neq j}$ for all $i,j\in\left\{ 1,\ldots,n\right\} $. 
\end{defn*}

\begin{defn*}
A matrix is \textbf{orthogonal }if it is a square matrix with orthonormal
columns. 

It follows from the definition that if a matrix $M\in\reals^{n\times n}$
is orthogonal, then $M^{T}M=I$, where $I$ is the $n\times n$ identity
matrix. Thus $M^{T}=M^{-1}$, and so $MM^{T}=I$ as well. 
\end{defn*}

\begin{defn*}
A matrix $M$ is \textbf{symmetric }if $M=M^{T}$. 
\end{defn*}

\begin{defn*}
For a square matrix $M$, if $Mv=\lambda v$ for some column vector
$v$ and scalar $\lambda$, then $v$ is called an \textbf{eigenvector}
of $M$ and $v$ is the corresponding \textbf{eigenvalue}. \end{defn*}
\begin{thm*}
[Spectral Theorem]A real, symmetric matrix $M\in\reals^{n\times n}$
can be diagonalized as $M=Q\Sigma Q^{T}$, where $Q\in\reals^{n\times n}$
is an orthogonal matrix whose columns are a set of orthonormal eigenvectors
of $M$, and $\Sigma$ is a diagonal matrix of the corresponding eigenvalues. \end{thm*}
\begin{defn*}
A real, symmetric matrix $M\in\reals^{n\times n}$ is \textbf{positive
semidefinite (psd)} if for any $x\in\reals^{n}$, 
\[
x^{T}Mx\ge0.
\]


Note that unless otherwise specified, when a matrix is described as
positive semidefinite, we are implicitly assuming it is real and symmetric
(or complex and Hermitian in certain contexts, though not here).

As an exercise in matrix multiplication, note that for any matrix
$A$ with columns $a_{1},\ldots,a_{d}$, that is 
\[
A=\begin{pmatrix}| &  & |\\
a_{1} & \cdots & a_{d}\\
| &  & |
\end{pmatrix}\in\reals^{n\times d},
\]
we have
\[
A^{T}MA=\begin{pmatrix}a_{1}^{T}Ma_{1} & a_{1}^{T}Ma_{2} & \cdots & a_{1}^{T}Ma_{d}\\
a_{2}^{T}Ma_{1} & a_{2}^{T}Ma_{2} & \cdots & a_{2}^{T}Ma_{d}\\
\vdots & \vdots & \cdots & \vdots\\
a_{d}^{T}Ma_{1} & a_{d}^{T}Ma_{2} & \cdots & a_{d}^{T}Ma_{d}
\end{pmatrix}.
\]
So $M$ is psd if and only if for any $A\in\reals^{n\times d}$, we
have $\diag(A^{T}MA)=\left(a_{1}^{T}Ma_{1},\ldots,a_{d}^{T}Ma_{d}\right)^{T}\succeq0$,
where $\succeq$ is elementwise inequality, and $0$ is a $d\times1$
column vector of $0$'s . \end{defn*}
\begin{enumerate}
\item Give an example of an orthogonal matrix that is not symmetric. (Hint:
You can use a $2\times2$ matrix with only $0$'s and $1$'s.) 
\item Use the definition of a psd matrix and the spectral theorem to show
that all eigenvalues of a positive semidefinite matrix are non-negative.
{[}Hint: In the definition of psd, make a good choice for $x$.{]} 
\item In this problem we show that a psd matrix is a matrix version of a
non-negative scalar, in that they both have a ``square root''. Show
that a symmetric matrix $M$ can be expressed as $M=BB^{T}$ for some
matrix $B$, if and only if $M$ is psd. {[}Hint: To show $M=BB^{T}$
implies $M$ is psd, use the fact that for any vector $v$, $v^{T}v\ge0$.
To show that $M$ psd implies $M=BB^{T}$ for some $B$, use the Spectral
Theorem.{]} 
\end{enumerate}

\section{Kernel Matrices}

(Problem from Michael Jordan's Stat 241b Problem Set \#1, Spring 2004)

The following problem will gives us some additional insight into what
information is encoded in the kernel matrix. 
\begin{enumerate}
\item Consider a set of vectors $S=\{x_{1},\ldots,x_{m}\}$. Let $X$ denote
the matrix whose rows are these vectors. Form the Gram matrix $K=XX^{T}$.
Show that knowing $K$ is equivalent to knowing the set of pairwise
distances among the vectors in $S$ as well as the vector lengths.
{[}Hint: The distance between $x$ and $y$ is given by $d(x,y)=\|x-y\|$,
and the norm of a vector $x$ is defined as $\|x\|=$$\sqrt{\left\langle x,x\right\rangle }=\sqrt{x^{T}x}$.{]} 
\end{enumerate}

\section{Kernel Ridge Regression}

In this problem, we complete the kernelization of ridge regression
that we discussed in Lab (see \url{https://davidrosenberg.github.io/ml2015/docs/4.Lab.kernelizations.pdf}).
To recap, the ridge regression objective function is given by
\[
J(w)=||Xw-y||^{2}+\lambda||w||^{2},
\]
where $X\in\reals^{n\times d}$ is the design matrix, and $\lambda>0$
is the regularization parameter. We showed that the minimizer is given
by $w^{*}=X^{T}\alpha^{*}$, where $\alpha^{*}=(\lambda I+XX^{T})^{-1}y$.
We can replace the Gram matrix $XX^{T}$ by the data kernel matrix
corresponding to any kernel satisfying Mercer's theorem. We also showed
that the vector of predictions on the training points is given by
$Xw=\left(XX^{T}\right)\left(\lambda I+XX^{T}\right)^{-1}y$, which
is also ``kernelized''. 
\begin{enumerate}
\item Give an expression for the prediction $f(x)=x^{T}w^{*}$ for a new
point $x$, not in the training set. The expression should only involve
$x$ via inner products with other $x$'s. {[}Hint: It is often convenient
to define the column vector
\[
k_{x}=\begin{pmatrix}x^{T}x_{1}\\
\vdots\\
x^{T}x_{n}
\end{pmatrix}
\]
to simplify the expression.{]} 
\end{enumerate}

\section{Novelty Detection}

(Problem derived from Michael Jordan's Stat 241b Problem Set \#2,
Spring 2004)

A novelty detection algorithm can be based on an algorithm that finds
the smallest possible sphere containing the data in feature space. 
\begin{enumerate}
\item Let $\phi:\cx\to F$ be our feature map, mapping elements of the input
space into our ``feature space'' $F$, which is equipped with an
inner product. Formulate the novelty detection algorithm described
above as an optimization problem. 
\item Give the Lagrangian for this problem, and write an equivalent, unconstrained
``$\inf\sup$'' version of the optimization problem.
\item Show that we have strong duality and thus we will have an equivalent
optimization problem if we swap the inf and the sup. {[}Hint: Use
Slater's qualification conditions.{]} 
\item Solve the inner minimization problem and give the dual optimization
problem. {[}Note: You may find it convenient to define the kernel
function $k(x_{i},x_{j})=\left\langle \phi(x_{i}),\phi(x_{j})\right\rangle $
and to write your final problem in terms of the corresponding kernel
matrix $K$ to simplify notation.{]} 
\item Write an expression for the optimal sphere in terms of the solution
to the dual problem. 
\item Write down the complementary slackness conditions for this problem,
and characterize the points that are the ``support vectors''.
\item Briefly explain how you would apply this algorithm in practice to
detect ``novel'' instances.
\item {[}Optional{]} Redo this problem allowing some of the data to lie
outside of the sphere, where the number of points outside the sphere
can be increased or decreased by adjusting a parameter. (Hint: Use
slack variables). 
\end{enumerate}

\section{Feedback (not graded)}
\begin{enumerate}
\item Approximately how long did it take to complete this assignment?
\item Any other feedback?\end{enumerate}

\end{document}
