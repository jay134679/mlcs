{
 "metadata": {
  "name": "",
  "signature": "sha256:803614d09ae3e2009caa51fdc5cd7725afacdab97f5a3bf3cdf69a9395f17b5d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Homework 4: Kernels and Duals\n",
      "\n",
      "## Alex Pine 2015/02/28"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2 Positive Semidefinite Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2.1 Give an example of an orthogonal matrix that is not symmetric.\n",
      "\\begin{align}\n",
      "\\begin{pmatrix}\n",
      "0 & 1 \\\\\n",
      "-1 & 0\n",
      "\\end{pmatrix}\n",
      "\\end{align}\n",
      "\n",
      "## 2.2 Use the definition of a PSD matrix and the spectral theorem to show that all eigenvalues of a positive semidefinite matrix are non-negative.\n",
      "Let $M$ be a PSD matrix.\n",
      "\n",
      "$M = Q \\Sigma Q^T$, where $\\Sigma$ is a diagonal matrix of eigenvalues, and $Q$ is an orthonormal matrix of eigenvectors. (Spectral thereom)\n",
      "\n",
      "Let $q$ be one of the columns of $Q$.\n",
      "\n",
      "$q^T M q \\geq 0$. (Definition of PSD)\n",
      "\n",
      "$q^T M q \\geq 0 \\implies q^T Q \\Sigma Q^T q \\geq 0$. (Substitution).\n",
      "\n",
      "Let $v = Q^T q$.\n",
      "\n",
      "$v \\succeq 0$, because the $i$th entry in $q = \\langle q_i, q_i \\rangle \\geq 0$, and all the others are 0 since $\\langle q_i, q_j \\rangle = 0$ when $i \\neq j$ since the columns of Q are orthonormal.  \n",
      "\n",
      "$q^T Q \\Sigma Q^T q \\geq 0 \\implies v^T \\Sigma v \\geq 0$. (Substitution)\n",
      "\n",
      "$v^T \\Sigma v \\geq 0$ and $v \\succeq 0 \\implies \\text{diag}(\\Sigma) \\succeq 0 $ (Matrix multiplication rules).\n",
      "\n",
      "$\\text{diag}(\\Sigma) \\succeq 0 \\implies$ all eigenvalues of $\\Sigma$ are positive, since $\\text{diag}(\\Sigma)$ are the eigenvalues of $M$.\n",
      "\n",
      "\n",
      "## 2.2  Show that a symmetric matrix M can be expressed as $M = BB^T$ for some matrix $B$, if and only if $M$ is PSD.\n",
      "### 2.2.1 Prove $M = BB^T \\implies M$ is PSD\n",
      "\n",
      "$x^T M x = x^T B B^T x$ (Since $M = BB^T$)\n",
      "\n",
      "Let $v = B^T x$. This implies $v^T = x^T B$ (Matrix transpose rules)\n",
      "\n",
      "$x^T B B^T x = v^T v$ (Substitution)\n",
      "\n",
      "$v^T v \\geq 0$ (Since each element at index $i$ is $v_i^2 \\geq 0 $)\n",
      "\n",
      "$v^T v \\geq 0 \\implies x^T M x \\geq 0$ (Substitution).\n",
      "\n",
      "$x^T M x \\geq 0 \\implies M$ is PSD. (Definition of PSD)\n",
      "\n",
      "### 2.2.2 Prove $M$ is PSD $\\implies M = BB^T$\n",
      "\n",
      "$M$ is PSD $\\implies$ M is real and symmetric. (Definition of PSD)\n",
      "\n",
      "M is real and symmetric $\\implies M = Q^T \\Sigma Q$, where $\\Sigma$ is a diagonal matrix of eigenvalues $\\lambda_i$, $i=1...n$. (Spectral Theorem)\n",
      "\n",
      "Let $R$ be a diagonial matrix where $\\text{diag}(R) = \\sqrt{\\lambda_i}$, $i=1...n$\n",
      "\n",
      "This definition implies $R = R^T$ and $R^T R = \\Sigma$.\n",
      "\n",
      "$R^T R = \\Sigma$ and $M = Q^T \\Sigma Q \\implies Q^T R^T R Q$. (Substitution)\n",
      "\n",
      "Let $B = Q^T R^T$, then $Q^T R^T R Q = B B^T$. (Transpose rules)\n",
      "\n",
      "$\\therefore M$ is PSD $\\implies M = BB^T$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 3 Kernal Matrices\n",
      "Let $K = X X^T$, where the $i$th row of $X$ is the $i$th vector of a set $S$.\n",
      "## 3.1 Show that knowing $K$ is equivalent to knowing the set of pairwise distances among the vectors in S as well as the vector lengths.\n",
      "The entry at row $i$ and column $j$ of $K$ is equal to $\\langle S_i, S_J \\rangle$. (Definition of $K$ and matrix multiplication rules).\n",
      "\n",
      "The length of vector $S_i$ is therefore equal to $\\sqrt{K_{i,i}}$.\n",
      "\n",
      "The distance between the vectors $S_i$ and $S_j$ equals $||S_i - S_J||$.\n",
      "\n",
      "$||S_i - S_J|| = \\sqrt{\\langle (S_i - S_J), (S_i - S_J) \\rangle} = \\sqrt{\\langle S_i, S_i \\rangle + \\langle S_j, S_j \\rangle - 2 \\langle S_i, S_j \\rangle}$ (Definition of inner project and multiplication)\n",
      "\n",
      "$\\sqrt{\\langle S_i, S_i \\rangle + \\langle S_j, S_j \\rangle - 2 \\langle S_i, S_j \\rangle} = \\sqrt{X_{i,i} + X_{j,j} - 2 X_{i,j}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 4 Kernel Ridge Regression\n",
      "## 4.1 Give an expression for the prediction $f(x) = x^T w^\u2217$ for a new point $x$, not in the training set.\n",
      " \n",
      "Let $X$ be the design matrix, where each row is an observation, and $y_i$ is the label for the $i$th observation.\n",
      "\n",
      "Since $w^* = X^T \\alpha^*$, then the prediction can be written $x^T w = x^T X^T \\alpha^* = k_x^T \\alpha^*$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 5 Novelty Detection\n",
      "\n",
      "## 5.1 Formulate the novelty detection algorithm described above as an optimization problem.\n",
      "\n",
      "Let $c \\in \\mathbb{R}^d$ be the center of the sphere, and let $r \\in \\mathbb{R}$ be its radius. The smallest sphere containing all vectors $x_i$ where $i=1,...,n$ is given by\n",
      "\n",
      "\\begin{align}\n",
      "\\min\\limits_{c, r} r^2, \\text{ where  } \\big(||\\phi(x) - c||^2 - r^2 \\big) \\leq 0 \\text{  and  } r \\geq 0\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 5.2 Give the Lagrangian for this problem, and write an equivalent, unconstrained \u201cinf sup\u201d version of the optimization problem.\n",
      "\n",
      "\\begin{align}\n",
      "L(c, r, \\lambda, \\nu) = r^2 + \\sum_{i=1}^{n} \\big[ \\lambda_i \\big(||\\phi(x) - c||^2 - r^2 \\big) \\big] - \\nu r\n",
      "\\end{align}\n",
      "\n",
      "The smallest sphere that encloses all the data points is given by\n",
      "\\begin{align}\n",
      "p^* = \\inf\\limits_{c,r}\\sup\\limits_{\\lambda, \\nu \\succeq 0} L(c, r, \\lambda, \\nu)\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 5.3 Show that we have strong duality and thus we will have an equivalent optimization problem if we swap the inf and the sup.\n",
      "\n",
      "Slater's condition states that we have strong duality if $\\exists \\phi(x) \\in \\mathcal{F}$ where $\\big(||\\phi(x) - c||^2 - r^2 \\big) < 0 \\text{  and  } r > 0 $\n",
      "\n",
      "The vector $\\phi(x) = c$ and $r = 1$ satisfy this, so we have strong duality:\n",
      "\n",
      "\\begin{align}\n",
      "p^* = \\inf\\limits_{c,r}\\sup\\limits_{\\lambda, \\nu \\succeq 0} L(c, r, \\lambda, \\nu) = \\sup\\limits_{\\lambda, \\nu \\succeq 0}\\inf\\limits_{c,r} L(c, r, \\lambda, \\nu) = d^*\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 5.4 Solve the inner minimization problem and give the dual optimization problem.\n",
      "TODO"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}