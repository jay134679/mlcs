{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "### Alex Pine\n",
    "### 2015/04/09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Robust Ridge Regression\n",
    "\n",
    "### 1.1 Write the objective function for $\\ell_2$-regularized emprical risk minimization with an $\\ell_1$ loss, over a linear hypothesis space.\n",
    "\n",
    "\\begin{align}\n",
    "J(w) = \\frac{1}{n}\\sum_{i=1}^{n}|x_{i}^{T} - y_i| + \\lambda\\|w\\|^2\n",
    "\\end{align}\n",
    "\n",
    "### 1.2 Show that the objective function is convex\n",
    " \n",
    "$x_{i}^{T} - y_i$ is an affine mapping, and the $L_1$ norm is convex. Therefore |$x_{i}^{T} - y_i|$ is convex, because it is a convex function composed with an affine mapping.\n",
    "\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}|x_{i}^{T} - y_i|$ is convex because it is the sum of weighted convex functions.\n",
    "\n",
    "$\\|w\\|$ is convex because it is a norm function. $\\|w\\|^2$ is convex because $\\|w\\|$ is positive, and a polynomial of a convex function is convex if the domain of the function is positive.\n",
    "\n",
    "Lastly, $\\frac{1}{n}\\sum_{i=1}^{n}|x_{i}^{T} - y_i| + \\lambda\\|w\\|^2$ is convex because it is the sum of two convex functions.\n",
    "\n",
    "### 1.3 Write the objective function as a quadratic program (i.e. an optimization problem with a quadratic objective and linear constraints).\n",
    "\n",
    "$J(w) = \\frac{1}{n}\\sum_{i=1}^{n}|x_{i}^{T} - y_i|$, subject to $\\|w\\|^2 \\le r, r \\ge 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 RBF Kernel\n",
    "\n",
    "### 2.1 Show that the distance between the feature representations of any two points $x_i$ and $x_j$ in the space H is at most $\\sqrt2$.\n",
    "\n",
    "Proof my contradiction. \n",
    "\n",
    "Assert $d(\\phi(x_i),\\phi(x_j)) > \\sqrt{2}$, then\n",
    "\n",
    "\\begin{align}\n",
    "\\|\\phi(x_i), \\phi(x_j)\\| & > \\sqrt{2}\n",
    "\\\\\n",
    "\\langle \\phi(x_i) - \\phi(x_j), \\phi(x_i) - \\phi(x_j)\\rangle & > 2\n",
    "\\\\\n",
    "\\langle \\phi(x_i), \\phi(x_i) \\rangle + \\langle \\phi(x_j), \\phi(x_j) \\rangle - 2\\langle \\phi(x_i), \\phi(x_j) \\rangle  & > 2\n",
    "\\\\\n",
    "2 - 2 k(x_i, x_j) & > 2\n",
    "\\\\\n",
    "k(x_i, x_j) & < 0\n",
    "\\\\\n",
    "\\text{exp}(-\\frac{1}{2} \\|x_i-x_j\\|^2) & < 0 \\implies \\text{impossible!}\n",
    "\\\\\n",
    "\\therefore d(\\phi(x_i),\\phi(x_j)) \\le \\sqrt{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Regularized Logistic Regression\n",
    "\n",
    "### 3.1 J(w) has multiple locally optimal solutions? T/F?\n",
    "\n",
    "False, J is convex, so it has one optimal solution. It's convex because its loss function and regularization term are convex. Its loss function is convex because the negative logarithm of a positive function is convex, and the sigmoid function is a positive function. Additionally, the regularization term is convex since it's a polynomial of a norm.\n",
    "\n",
    "### 3.2 Let $\\hat{w}$ = $\\text{argmin}_w J(w)$ be a global optimum. Then $\\hat{w}$ is sparse (i.e. has many zero entries). T/F?\n",
    "\n",
    "False, $L_2$ regularization does not lead to sparse solutions. \n",
    "\n",
    "### 3.3 . [Optional] If the training data are linearly separable, then some weights $w_j$ might become infinite if λ = 0. T/F?\n",
    "\n",
    "True, if there is no regularization term, there is nothing to prevent the weights from growing to increase the margin at each iteration of optimization.. \n",
    "\n",
    "### 3.4 $L(w, D_\\text{train})$ always increases as we increase λ. [NOTE: L is the log-likelihood, and the negative empirical risk.] T/F?\n",
    "\n",
    "False, $L(w, D_\\text{train})$ will DECREASE as lambda increases. As lambda increases, J can only be maintained by decreasing the $L_2$ norm of w. Since an unconstrained w maximizes $L(w, D_\\text{train})$, changing w must decrease it.\n",
    "\n",
    "### 3.5 $L(w, D_\\text{test})$ always increases as we increase λ. T/F?\n",
    "\n",
    "False. This is typically true, but it depends on the test set. For example, if the test set is identical to the training set with a tiny term added to each datapoint, a zero lambda will lead to a w that causes $L(w, D_\\text{test})$ to be larger than a large lambda would."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Neural Networks with Linear Activation Function\n",
    "\n",
    "### 4.1 Redesign the neural network to compute the same function without using any hidden units. Draw the equivalent network and give expressions detailing for the new weights in terms of the old weights and the constant c.\n",
    "\n",
    "\n",
    "$w_a = c(w_1 w_5 + w_2 w_6)$\n",
    "\n",
    "$w_b = c(w_3 w_5 + w_4 w_6)$\n",
    "\n",
    "\n",
    "$y = w_a x_1 + w_b x_2$\n",
    "\n",
    "\n",
    "### 4.2 Can the space of functions that is represented by the above neural network also be represented by linear regression?\n",
    "\n",
    "Yes. After simplifying it, the decisino function is a linearly combination of the elements of $x$, which is exactly what linear regression does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
