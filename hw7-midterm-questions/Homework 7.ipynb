{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "### Alex Pine, 2015/04/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Ivanov and Tikhonov Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1.1 Tikhonov optimal implies Ivanov optimal\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ivanov optimal implies Tikhonov optimal\n",
    "#### 1.2.1 Ivanov Lagrangian\n",
    "\\begin{align}\n",
    "L(w, \\lambda) = \\phi(w) + \\lambda(\\Omega(w) - r)\n",
    "\\end{align}\n",
    "\n",
    "#### 1.2.2 Write the dual optimization problem in terms of the dual objective function $g(\\lambda)$, and give an expression for $g(\\lambda)$.\n",
    "\n",
    "\\begin{align}\n",
    "d^* & = \\text{sup}_{\\lambda \\succcurlyeq 0}g(\\lambda)\n",
    "\\\\\n",
    "g(\\lambda) & = \\text{inf}_w L(w, \\lambda) = \\text{inf}_w \\big[ \\phi(w) + \\lambda(\\Omega(w) - r) \\big]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1.3 Ivanov implies Tikhonov for Ridge Regression\n",
    "\n",
    "\\begin{align}\n",
    "g(\\lambda^*) & = \\text{inf}_w \\big[ \\phi(w) + \\lambda^*(\\Omega(w) - r) \\big]\n",
    "\\\\\n",
    "g(\\lambda^*) & = \\phi(w^*) + \\lambda^*(\\Omega(w^*) - r)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Since strong duality holds for this problem, complementary slackness holds too, which implies that $\\lambda^*(\\Omega(w^*) - r) = 0$, therefore,\n",
    "\n",
    "\\begin{align}\n",
    "g(\\lambda^*) & = \\phi(w^*)\n",
    "\\end{align}\n",
    "\n",
    "Strong duality implies that when $\\lambda^* \\ne 0$, then $\\Omega(w^*) = r$ must be true. In that case,  \n",
    "\n",
    "\\begin{align}\n",
    "w^* = \\text{argmin}_w \\big[ \\phi(w) + \\lambda^*\\Omega(w) \\big]\n",
    "\\end{align}\n",
    "\n",
    "TODO TODO TODO why????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Square Hinge Loss and Huberized Square Hinge Loss\n",
    "\n",
    "### 2.1 Write the objective function J(w) for L2-regularized empirical risk minimization with the square hinge loss.\n",
    "\\begin{align}\n",
    "J(w) = [(1-y w^T x)_+]^2 + \\lambda\\|w\\|^2\n",
    "\\end{align}\n",
    "\n",
    "### 2.2 Give the derivative of J(w).\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla J_w (w) = [-2(1-y w^T x)]_+ + \\lambda w\n",
    "\\end{align}\n",
    "\n",
    "### TODO 2.3 Give pseudocode or otherwise explain how you would use stochastic gradient descent to find $w^∗$. You need to specify your approach to the step size.\n",
    "TODO Why is step size special here?\n",
    "\n",
    "$m_i = y_i(w_{i-1}^T x_i)$ \n",
    "\n",
    "if $m < 1: w_i ← w_{i-1}(1-\\eta_i\\lambda) + 2\\eta_i (1-m_i)$\n",
    "\n",
    "else: $w_i ← w_{i-1}(1-\\eta_i\\lambda)$\n",
    "\n",
    "\n",
    "### 2.4 Justify the claim that the output of SGD can be written in the form: $w = \\sum^{n}_{i=1}\\beta_i x_i$.\n",
    "Each update of w is a linear combination of the previous value of w and a multiple of the current iteration's datapoint, $(x_i, y_i)$. Since we intialize w at the zero vector, it follows by induction that w is a linear combination of $(x_i, y_i)$ pairs. \n",
    "\n",
    "That is, $w = \\sum^{n}_{i=1}\\alpha_i y_i x_i$. If we set $\\beta_i = \\alpha_i y_i$, then we get $w = \\sum^{n}_{i=1}\\beta_i x_i$.\n",
    "\n",
    "### 2.5 In relation to the SGD algorithm, how would you characterize the $x_i$’s that are support vectors?\n",
    "They are the $(x_i, y_i)$ pairs for which $y_i(w_{i-1}^T x_i) < 1$. This is interesting, because the ordering of the $(x_i, y_i)$ pairs partially determines which ones become support vectors.\n",
    "\n",
    "### 2.6 Show that J(w) is convex.\n",
    "\n",
    "$J(w)$ can be written as  $J(w) = [\\text{max}(0, 1-y (w^T x))]^2 + \\lambda\\|w\\|^2$. We'll split this into the two cases of the max() function:\n",
    "\n",
    "1) When $y_i(w_{i-1}^T x_i) < 1$, $J(w) = [(1-y w^T x)]^2 + \\lambda\\|w\\|^2$.\n",
    "\n",
    "Functions $f(x) = x^a$ are convex when $x \\in R^{++}$ and $a > 1$. Since $(1-y w^T x) > 0$, $[(1-y w^T x)]^2$ (a.k.a. $\\ell(w)$) is convex. Since norm functions are convex and positive, $\\lambda\\|w\\|^2$ is convex for the same reason $\\ell(w)$ is. Since weighted sums of convex functions are convex, $J(w)$ is convex.\n",
    "\n",
    "2) When $y_i(w_{i-1}^T x_i) \\ge 1$, $J(w) = \\lambda\\|w\\|^2$.\n",
    "\n",
    "The convexity of $\\lambda\\|w\\|^2$ was proven in the last section.\n",
    "\n",
    "Since the maximum of multiple convex functions is convex, $J(w)$ is convex.\n",
    "\n",
    "### TODO 2.7 When might you prefer the Huberized square hinge loss to the square hinge loss?\n",
    "The second derivative of squared hinge is not differentiable. Maybe that matters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conditional Exponential Distributions\n",
    "### 3.1 Write the family of exponential distributions as a natural exponential family.\n",
    "\\begin{align}\n",
    "\\theta & = -\\lambda\n",
    "\\\\\n",
    "h(y) & = 1\n",
    "\\\\\n",
    "Z(\\theta) & = \\int_{y}h(y)e^{\\theta y} = \\int_{y}e^{-\\lambda y} = \\frac{1}{\\lambda} = -\\frac{1}{\\theta}\n",
    "\\\\\n",
    "p_{\\theta}(y) & = -\\theta e^{\\theta^T y}\n",
    "\\end{align}\n",
    "\n",
    "### 3.2  Suggest a reasonable function ψ to map $w^T x$ to a value in the natural parameter space Θ. Then write an expression for $p_w(y | x)$, the predicted probability density function conditioned on x.\n",
    "\n",
    "Let $\\psi(w^T x) = -w^T x$. \n",
    "\n",
    "This makes $Z(\\psi(w^{T}x)) = \\frac{1}{w^T x}$. Which makes $p_{w}(y | x) = w^Txe^{-(w^T x) y}$.\n",
    "\n",
    "### 3.3 Give the optimization problem you would solve to fit the GLM we have been discussing to training data.\n",
    "\\begin{align}\n",
    "w^* = \\text{argmin}_w \\sum_{i=1}^{n}\\big[log(w^T x_i) - (w^T x_i) y_i\\big]\n",
    "\\end{align}\n",
    "\n",
    "### TODO 3.4 (Optional) Explain how you would use gradient boosting in this situation. For partial credit, present another reasonable approach to this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
