{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "### Alex Pine, 2015/04/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Ivanov and Tikhonov Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.1 Tikhonov optimal implies Ivanov optimal\n",
    "\n",
    "Let's assume there does not exist any $r$ for which $f^* = \\text{argmin}_f \\big[ \\phi(f)$, subject to $\\Omega(f) < r \\big]$. \n",
    "This implies there exists some $f_2$ s.t. $f_2 \\ne f^*$ and $f_2 = \\text{argmin}_f \\big[ \\phi(f)$, subject to $\\Omega(f) < r \\big]$, for some non-negative $r$. \n",
    "\n",
    "This implies $\\phi(f_2) < \\phi(f^*)$. \n",
    "\n",
    "$\\Omega(f_2) < r$, per definition of $f_2$. Let $r = \\Omega(f^*)$, then $\\Omega(f_2) < \\Omega(f^*)$. Putting these together with a non-negative $\\lambda$, we get\n",
    "\n",
    "\\begin{align}\n",
    "\\phi(f_2) + \\lambda \\Omega(f^*) & < \\phi(f^*) + \\lambda \\Omega(f^*)\n",
    "\\\\\n",
    "\\phi(f_2) + \\lambda \\Omega(f_2) < \\phi(f_2) + \\lambda \\Omega(f^*) & < \\phi(f^*) + \\lambda \\Omega(f^*)\n",
    "\\\\\n",
    "\\phi(f_2) + \\lambda \\Omega(f_2) & < \\phi(f^*) + \\lambda \\Omega(f^*)\n",
    "\\end{align}\n",
    "\n",
    "This implies that $f^* \\ne \\text{argmin}_f \\big[ \\phi(f) + \\lambda \\Omega(f) \\big]$, which is a contradiction of the definition of $f^*$.\n",
    "\n",
    "Therefore $f_* = \\text{argmin}_f \\big[ \\phi(f)$, subject to $\\Omega(f) < r \\big]$ for some non-negative r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ivanov optimal implies Tikhonov optimal\n",
    "#### 1.2.1 Ivanov Lagrangian\n",
    "\\begin{align}\n",
    "L(w, \\lambda) = \\phi(w) + \\lambda(\\Omega(w) - r)\n",
    "\\end{align}\n",
    "\n",
    "#### 1.2.2 Write the dual optimization problem in terms of the dual objective function $g(\\lambda)$, and give an expression for $g(\\lambda)$.\n",
    "\n",
    "\\begin{align}\n",
    "d^* & = \\text{sup}_{\\lambda \\succcurlyeq 0}g(\\lambda)\n",
    "\\\\\n",
    "g(\\lambda) & = \\text{inf}_w L(w, \\lambda) = \\text{inf}_w \\big[ \\phi(w) + \\lambda(\\Omega(w) - r) \\big]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Ivanov implies Tikhonov for Ridge Regression\n",
    "\n",
    "\\begin{align}\n",
    "g(\\lambda^*) & = \\text{inf}_w \\big[ \\phi(w) + \\lambda^*(\\Omega(w) - r) \\big]\n",
    "\\\\\n",
    "g(\\lambda^*) & = \\phi(w^*) + \\lambda^*(\\Omega(w^*) - r)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Since strong duality holds for this problem, complementary slackness holds too, which implies that $\\lambda^*(\\Omega(w^*) - r) = 0$, therefore,\n",
    "\n",
    "\\begin{align}\n",
    "g(\\lambda^*) & = \\phi(w^*)\n",
    "\\end{align} \n",
    "\n",
    "By definition of the argmin expression, this implies\n",
    "\n",
    "\\begin{align}\n",
    "\\phi(w^*) & = \\text{inf}_w \\big[ \\phi(w) + \\lambda^*(\\Omega(w) - r) \\big]\n",
    "\\\\\n",
    "w^* & = \\text{argmin}_w \\big[ \\phi(w^*) + \\lambda^*(\\Omega(w^*) - r) \\big]\n",
    "\\\\\n",
    "w^* & = \\text{argmin}_w \\big[ \\phi(w^*) + \\lambda^* \\Omega(w^*) - \\lambda^* r \\big]\n",
    "\\end{align} \n",
    "\n",
    "Since the $-\\lambda^* r$ term is independent of $w$, the following must also be true:\n",
    "\n",
    "\\begin{align}\n",
    "w^* & = \\text{argmin}_w \\big[ \\phi(w) + \\lambda^*\\Omega(w) \\big]\n",
    "\\end{align}\n",
    "\n",
    "This is the result we were trying to prove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Square Hinge Loss and Huberized Square Hinge Loss\n",
    "\n",
    "### 2.1 Write the objective function J(w) for L2-regularized empirical risk minimization with the square hinge loss.\n",
    "\\begin{align}\n",
    "J(w) = [(1-y w^T x)_+]^2 + \\lambda\\|w\\|^2\n",
    "\\end{align}\n",
    "\n",
    "### 2.2 Give the derivative of J(w).\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla J_w (w) = [-2(1-y w^T x)]_+ + \\lambda w\n",
    "\\end{align}\n",
    "\n",
    "### 2.3 Give pseudocode or otherwise explain how you would use stochastic gradient descent to find $w^∗$. You need to specify your approach to the step size.\n",
    "At each iteration of subgradient descent, the update rule would be:\n",
    "\n",
    "\\begin{align}\n",
    "& m_i = y_i(w_{i-1}^T x_i)\n",
    "\\\\\n",
    "& \\text{if } m < 1: w_i ← w_{i-1}(1-\\eta_i\\lambda) + 2\\eta_i (1-m_i)\n",
    "\\\\\n",
    "& \\text{else: } w_i ← w_{i-1}(1-\\eta_i\\lambda)\n",
    "\\end{align}\n",
    "\n",
    "Where $i$ is the iteration number (starting from 1), and the step size is $\\eta_i = \\frac{1}{\\lambda i}$. \n",
    "\n",
    "### 2.4 Justify the claim that the output of SGD can be written in the form: $w = \\sum^{n}_{i=1}\\beta_i x_i$.\n",
    "Each update of w is a linear combination of the previous value of w and a multiple of the current iteration's datapoint, $(x_i, y_i)$. Since we intialize w at the zero vector, it follows by induction that w is a linear combination of $(x_i, y_i)$ pairs. \n",
    "\n",
    "That is, $w = \\sum^{n}_{i=1}\\alpha_i y_i x_i$. If we set $\\beta_i = \\alpha_i y_i$, then we get $w = \\sum^{n}_{i=1}\\beta_i x_i$.\n",
    "\n",
    "### 2.5 In relation to the SGD algorithm, how would you characterize the $x_i$’s that are support vectors?\n",
    "They are the $(x_i, y_i)$ pairs for which $y_i(w_{i-1}^T x_i) < 1$. This is interesting, because the ordering of the $(x_i, y_i)$ pairs partially determines which ones become support vectors.\n",
    "\n",
    "### 2.6 Show that J(w) is convex.\n",
    "\n",
    "$J(w)$ can be written as  $J(w) = [\\text{max}(0, 1-y (w^T x))]^2 + \\lambda\\|w\\|^2$. We'll split this into the two cases of the max() function:\n",
    "\n",
    "1) When $y_i(w_{i-1}^T x_i) < 1$, $J(w) = [(1-y w^T x)]^2 + \\lambda\\|w\\|^2$.\n",
    "\n",
    "Functions $f(x) = x^a$ are convex when $x \\in R^{++}$ and $a > 1$. Since $(1-y w^T x) > 0$, $[(1-y w^T x)]^2$ (a.k.a. $\\ell(w)$) is convex. Since norm functions are convex and positive, $\\lambda\\|w\\|^2$ is convex for the same reason $\\ell(w)$ is. Since weighted sums of convex functions are convex, $J(w)$ is convex.\n",
    "\n",
    "2) When $y_i(w_{i-1}^T x_i) \\ge 1$, $J(w) = \\lambda\\|w\\|^2$.\n",
    "\n",
    "The convexity of $\\lambda\\|w\\|^2$ was proven in the last section.\n",
    "\n",
    "Since the maximum of multiple convex functions is convex, $J(w)$ is convex.\n",
    "\n",
    "### 2.7 When might you prefer the Huberized square hinge loss to the square hinge loss?\n",
    "If your dataset has a large number of outliers, Huberized square hinge loss might be preferred. Normal square loss will give outliers a large influence on $w$ compared to the non-outlier points, which may not be desirable, especially if the outliers represent measurement errors or noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conditional Exponential Distributions\n",
    "### 3.1 Write the family of exponential distributions as a natural exponential family.\n",
    "\\begin{align}\n",
    "\\theta & = -\\lambda\n",
    "\\\\\n",
    "h(y) & = 1\n",
    "\\\\\n",
    "Z(\\theta) & = \\int_{y}h(y)e^{\\theta y} = \\int_{y}e^{-\\lambda y} = \\frac{1}{\\lambda} = -\\frac{1}{\\theta}\n",
    "\\\\\n",
    "p_{\\theta}(y) & = -\\theta e^{\\theta^T y}\n",
    "\\end{align}\n",
    "\n",
    "### 3.2  Suggest a reasonable function ψ to map $w^T x$ to a value in the natural parameter space Θ. Then write an expression for $p_w(y | x)$, the predicted probability density function conditioned on x.\n",
    "\n",
    "$Z(\\theta) < \\infty$ when $\\theta \\ne 0$, so we must choose $\\psi(w^T x)$ s.t. $\\psi(w^T x) \\ne 0$.\n",
    "\n",
    "Let $\\psi(w^T x) = -e^{w^T x}$. \n",
    "\n",
    "This makes $Z(\\psi(w^{T}x)) = \\frac{1}{e^{w^T x}} = e^{-w^T x}$. \n",
    "\n",
    "This makes $p_{w}(y | x) = e^{w^T x} \\text{exp}(-e^{w^T x} y) = \\text{exp}(w^T x - e^{w^T x} y) $.\n",
    "\n",
    "### 3.3 Give the optimization problem you would solve to fit the GLM we have been discussing to training data.\n",
    "\\begin{align}\n",
    "w^* = \\text{argmin}_w \\sum_{i=1}^{n}\\big[w^T x_i - e^{w^T x_i} y_i\\big]\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
